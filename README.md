# Lingo - LLM Serving

Autoscaling LLM serving on K8s

Insert demo gif

üöÄ Serve popular OSS LLM models in minutes on CPUs or GPUs  
‚öñÔ∏è  Automatically scale up and down, all the way to 0  
‚¨ÜÔ∏è  Built-in gateway that batches requests while scaling happens  
üõ†Ô∏è Easy to install, No complex dependencies such as Istio or Knative  
‚òÅÔ∏è  Provide a unified API across clouds for serving LLMs  

Support the project by adding a star! ‚ù§Ô∏è

Join us on Discord:  
<a href="https://discord.gg/JeXhcmjZVm">
<img alt="discord-invite" src="https://dcbadge.vercel.app/api/server/JeXhcmjZVm?style=flat">
</a>

## Quickstart
Install Lingo with default models:
```bash
helm repo add substratusai https://substratusai.github.io/helm
helm install lingo substratusai/lingo --wait
```

Lingo will be installed with various LLMs by default. You
can see the available models by running:
```bash
kubectl get deployment -l lingo-model
```
The deployments all have 0 replicas by default.

You can expose the Lingo Gateway service externally
or setup a port-forward. For the quickstart, we're
using port-forwarding:
```bash
kubectl port-forward service/lingo-gateway 8080:80
```

Send a request to the mistral-7b-instruct model that
was deployed:
```bash
url http://localhost:8080/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "mistralai/Mistral-7B-Instruct-v0.1", "prompt": "<s>[INST]Who was the first president of the United States?[/INST]", "max_tokens": 40}'
```
The first request can take up to 10 minutes,
because of potentially node provisioning, pulling the
container image and downloading the model before it can
serve the request.

## Architecture
TODO

## Creators
Feel free to contact any of us:
* [Nick Stogner](https://www.linkedin.com/in/nstogner/)
* [Sam Stoelinga](https://www.linkedin.com/in/samstoelinga/)
